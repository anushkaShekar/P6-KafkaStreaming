{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5b0bae-f548-40ac-8f18-afa588ba501e",
   "metadata": {},
   "source": [
    "# Part 1: Kafka Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7797d2f2-b2cf-483a-9c8d-b32919ab836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, time, random, string\n",
    "\n",
    "def one_station(name):\n",
    "    # temp pattern\n",
    "    month_avg = [27,31,44,58,70,79,83,81,74,61,46,32]\n",
    "    shift = (random.random()-0.5) * 30\n",
    "    month_avg = [m + shift + (random.random()-0.5) * 5 for m in month_avg]\n",
    "    \n",
    "    # rain pattern\n",
    "    start_rain = [0.1,0.1,0.3,0.5,0.4,0.2,0.2,0.1,0.2,0.2,0.2,0.1]\n",
    "    shift = (random.random()-0.5) * 0.1\n",
    "    start_rain = [r + shift + (random.random() - 0.5) * 0.2 for r in start_rain]\n",
    "    stop_rain = 0.2 + random.random() * 0.2\n",
    "\n",
    "    # day's state\n",
    "    today = datetime.date(2000, 1, 1)\n",
    "    temp = month_avg[0]\n",
    "    raining = False\n",
    "    \n",
    "    # gen weather\n",
    "    while True:\n",
    "        # choose temp+rain\n",
    "        month = today.month - 1\n",
    "        temp = temp * 0.8 + month_avg[month] * 0.2 + (random.random()-0.5) * 20\n",
    "        if temp < 32:\n",
    "            raining=False\n",
    "        elif raining and random.random() < stop_rain:\n",
    "            raining = False\n",
    "        elif not raining and random.random() < start_rain[month]:\n",
    "            raining = True\n",
    "\n",
    "        yield (today.strftime(\"%Y-%m-%d\"), name, temp, raining)\n",
    "\n",
    "        # next day\n",
    "        today += datetime.timedelta(days=1)\n",
    "        \n",
    "def all_stations(count=10, sleep_sec=1):\n",
    "    assert count <= 26\n",
    "    stations = []\n",
    "    for name in string.ascii_uppercase[:count]:\n",
    "        stations.append(one_station(name))\n",
    "    while True:\n",
    "        for station in stations:\n",
    "            yield next(station)\n",
    "        time.sleep(sleep_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35373f3b-5280-4215-86ea-735f7678f4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stations-json', 'stations']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer, TopicPartition\n",
    "from kafka.admin import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError, UnknownTopicOrPartitionError\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers=[\"kafka:9092\"])\n",
    "try:\n",
    "    admin.delete_topics([\"stations\", \"stations-json\"])\n",
    "    print(\"deleted\")\n",
    "except UnknownTopicOrPartitionError:\n",
    "    print(\"cannot delete (may not exist yet)\")\n",
    "\n",
    "time.sleep(1)\n",
    "admin.create_topics([NewTopic(\"stations\", 6, 1)])\n",
    "admin.create_topics([NewTopic(\"stations-json\", 6, 1)])\n",
    "admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275a6d73-0d04-4622-99d2-ff974016d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building protobuf file\n",
    "! python3 -m grpc_tools.protoc -I=. --python_out=. stations.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e60c7dbc-2de2-45f9-8e60-1c6aa7319a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, threading\n",
    "from stations_pb2 import *\n",
    "\n",
    "def produce():\n",
    "    producer = KafkaProducer(bootstrap_servers=[\"kafka:9092\"], retries=10, acks='all')\n",
    "    \n",
    "    for date, station, degrees, raining in all_stations(15):\n",
    "        # protobuf\n",
    "        r = Report(date=date, station=station, degrees=degrees, raining=raining)\n",
    "        value1 = r.SerializeToString()\n",
    "        key = bytes(station, \"utf-8\")\n",
    "        producer.send(\"stations\", value=value1, key=key)\n",
    "        \n",
    "        # JSON\n",
    "        value2 = {\"date\":date, \"station\":station, \"degrees\":degrees, \"raining\":raining}\n",
    "        \n",
    "        if raining == False:\n",
    "            value2[\"raining\"] = 0\n",
    "        else:\n",
    "            value2[\"raining\"] = 1\n",
    "            \n",
    "        value2 = bytes(json.dumps(value2), \"utf-8\")\n",
    "        producer.send(\"stations-json\", value=value2, key=key)\n",
    "        \n",
    "# never join thread because we want it to run forever\n",
    "threading.Thread(target=produce, args=()).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68396e-aa0b-4f6a-bf51-e886f814dc56",
   "metadata": {},
   "source": [
    "# Part 2: Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c79677e-d0d6-448e-9f6d-fc672563e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "for partition in range(6):\n",
    "    path = f\"partition-{partition}.json\"\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "568548b7-1214-4e1b-8dbf-3bc0a545c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and saving a partition\n",
    "def load_partition(partition_num):\n",
    "    path = f\"partition-{partition_num}.json\"\n",
    "    if os.path.exists(path):\n",
    "        print(\"TRUE\")\n",
    "        with open(path, \"r\") as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        return {\"offset\":None, \"partition\":partition_num}\n",
    "    \n",
    "def save_partition(partition):\n",
    "    path = f\"partition-{partition['partition']}.json\"\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump(partition, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a72c0035-b8f2-4873-84d5-9d43e829c434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND 0\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "exiting\n",
      "exiting\n",
      "exiting\n",
      "ROUND 1\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "exiting\n",
      "exiting\n",
      "exiting\n"
     ]
    }
   ],
   "source": [
    "def consume(part_nums=[], iterations=10):\n",
    "    consumer = KafkaConsumer(bootstrap_servers=[\"kafka:9092\"])\n",
    "    consumer.assign([TopicPartition(\"stations\", part_num) for part_num in part_nums])\n",
    "\n",
    "    # PART 1: initialization\n",
    "    partitions = {} # key=partition num, value=snapshot dict\n",
    "    for partition_num in part_nums:\n",
    "        res = load_partition(partition_num)\n",
    "        if res[\"offset\"] == None:\n",
    "            consumer.seek_to_beginning()\n",
    "        else:\n",
    "            offset = res[\"offset\"]\n",
    "            consumer.seek(TopicPartition(\"stations\", partition_num), offset)\n",
    "        save_partition(res)\n",
    "        partitions[partition_num] = load_partition(partition_num)\n",
    "        \n",
    "    # PART 2: process batches\n",
    "    for i in range(iterations):\n",
    "        batch = consumer.poll(1000) # 1s timeout\n",
    "        for topic, messages in batch.items():\n",
    "            temp_count = 0\n",
    "            temp_sum = 0\n",
    "            for msg in messages:\n",
    "                r = Report.FromString(msg.value)\n",
    "                partition = msg.partition\n",
    "                station_key = r.station\n",
    "                temp = r.degrees\n",
    "                current_offset = consumer.position(topic)\n",
    "                \n",
    "                temp_count += 1\n",
    "                temp_sum += temp\n",
    "                current_date = r.date\n",
    "\n",
    "                if station_key not in partitions[partition]:\n",
    "                    partitions[partition][station_key] = {}\n",
    "                    partitions[partition][station_key][\"start\"] = current_date\n",
    "                    partitions[partition][station_key][\"count\"] = temp_count\n",
    "                    partitions[partition][station_key][\"sum\"] = temp_sum\n",
    "                    partitions[partition][station_key][\"avg\"] = 0\n",
    "                else:\n",
    "                    previous_date = partitions[partition][station_key][\"end\"]\n",
    "                    if current_date <= previous_date:\n",
    "                        continue\n",
    "                    \n",
    "                partitions[partition][station_key][\"count\"] += temp_count\n",
    "                partitions[partition][station_key][\"sum\"] += temp_sum\n",
    "                partitions[partition][station_key][\"avg\"] = partitions[partition][station_key][\"sum\"] / partitions[partition][station_key][\"count\"]\n",
    "                partitions[partition][station_key][\"end\"] = current_date\n",
    "                partitions[partition][\"offset\"] = current_offset\n",
    "                \n",
    "        for part_num in partitions:\n",
    "            save_partition(partitions[partition])\n",
    "    print(\"exiting\")\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"ROUND\", i)\n",
    "    t1 = threading.Thread(target=consume, args=([0,1], 30))\n",
    "    t2 = threading.Thread(target=consume, args=([2,3], 30))\n",
    "    t3 = threading.Thread(target=consume, args=([4,5], 30))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7fcce6e-ebf6-4d75-a329-71a23b10d55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"offset\": 51, \"partition\": 0, \"N\": {\"start\": \"2000-01-01\", \"count\": 97, \"sum\": 3506.065653203066, \"avg\": 36.14500673405223, \"end\": \"2000-02-20\"}}{\"offset\": 104, \"partition\": 1, \"E\": {\"start\": \"2000-01-01\", \"count\": 143, \"sum\": 5411.817150538535, \"avg\": 37.84487517859115, \"end\": \"2000-02-21\"}, \"O\": {\"start\": \"2000-01-01\", \"count\": 195, \"sum\": 7553.525872849768, \"avg\": 38.7360301171783, \"end\": \"2000-02-21\"}}{\"offset\": 156, \"partition\": 2, \"F\": {\"start\": \"2000-01-01\", \"count\": 206, \"sum\": 5347.019945044205, \"avg\": 25.95640750021459, \"end\": \"2000-02-21\"}, \"I\": {\"start\": \"2000-01-01\", \"count\": 258, \"sum\": 6819.472360974371, \"avg\": 26.432063414629347, \"end\": \"2000-02-21\"}, \"J\": {\"start\": \"2000-01-01\", \"count\": 308, \"sum\": 7513.750462862488, \"avg\": 24.395293710592494, \"end\": \"2000-02-21\"}}{\"offset\": 162, \"partition\": 3, \"D\": {\"start\": \"2000-01-01\", \"count\": 230, \"sum\": 8031.075048435401, \"avg\": 34.917717601893045, \"end\": \"2000-02-23\"}, \"G\": {\"start\": \"2000-01-01\", \"count\": 283, \"sum\": 10277.894333487817, \"avg\": 36.31764782151172, \"end\": \"2000-02-23\"}, \"M\": {\"start\": \"2000-01-01\", \"count\": 334, \"sum\": 11921.479307972588, \"avg\": 35.69305182027721, \"end\": \"2000-02-23\"}}{\"offset\": 268, \"partition\": 4, \"A\": {\"start\": \"2000-01-01\", \"count\": 285, \"sum\": 6393.672966197566, \"avg\": 22.43394023227216, \"end\": \"2000-02-23\"}, \"B\": {\"start\": \"2000-01-01\", \"count\": 339, \"sum\": 7636.546058990681, \"avg\": 22.526684539795518, \"end\": \"2000-02-23\"}, \"C\": {\"start\": \"2000-01-01\", \"count\": 394, \"sum\": 8398.806621097261, \"avg\": 21.316768073850916, \"end\": \"2000-02-23\"}, \"K\": {\"start\": \"2000-01-01\", \"count\": 436, \"sum\": 9455.428404100952, \"avg\": 21.686762394726955, \"end\": \"2000-02-22\"}, \"L\": {\"start\": \"2000-01-01\", \"count\": 490, \"sum\": 11391.867116942713, \"avg\": 23.248708401923903, \"end\": \"2000-02-22\"}}{\"offset\": 53, \"partition\": 5, \"H\": {\"start\": \"2000-01-01\", \"count\": 102, \"sum\": 3903.634534583164, \"avg\": 38.270926809638866, \"end\": \"2000-02-22\"}}"
     ]
    }
   ],
   "source": [
    "!cat partition*.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
