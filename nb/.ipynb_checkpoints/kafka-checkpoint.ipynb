{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5b0bae-f548-40ac-8f18-afa588ba501e",
   "metadata": {},
   "source": [
    "# Part 1: Kafka Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7797d2f2-b2cf-483a-9c8d-b32919ab836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, time, random, string\n",
    "\n",
    "def one_station(name):\n",
    "    # temp pattern\n",
    "    month_avg = [27,31,44,58,70,79,83,81,74,61,46,32]\n",
    "    shift = (random.random()-0.5) * 30\n",
    "    month_avg = [m + shift + (random.random()-0.5) * 5 for m in month_avg]\n",
    "    \n",
    "    # rain pattern\n",
    "    start_rain = [0.1,0.1,0.3,0.5,0.4,0.2,0.2,0.1,0.2,0.2,0.2,0.1]\n",
    "    shift = (random.random()-0.5) * 0.1\n",
    "    start_rain = [r + shift + (random.random() - 0.5) * 0.2 for r in start_rain]\n",
    "    stop_rain = 0.2 + random.random() * 0.2\n",
    "\n",
    "    # day's state\n",
    "    today = datetime.date(2000, 1, 1)\n",
    "    temp = month_avg[0]\n",
    "    raining = False\n",
    "    \n",
    "    # gen weather\n",
    "    while True:\n",
    "        # choose temp+rain\n",
    "        month = today.month - 1\n",
    "        temp = temp * 0.8 + month_avg[month] * 0.2 + (random.random()-0.5) * 20\n",
    "        if temp < 32:\n",
    "            raining=False\n",
    "        elif raining and random.random() < stop_rain:\n",
    "            raining = False\n",
    "        elif not raining and random.random() < start_rain[month]:\n",
    "            raining = True\n",
    "\n",
    "        yield (today.strftime(\"%Y-%m-%d\"), name, temp, raining)\n",
    "\n",
    "        # next day\n",
    "        today += datetime.timedelta(days=1)\n",
    "        \n",
    "def all_stations(count=10, sleep_sec=1):\n",
    "    assert count <= 26\n",
    "    stations = []\n",
    "    for name in string.ascii_uppercase[:count]:\n",
    "        stations.append(one_station(name))\n",
    "    while True:\n",
    "        for station in stations:\n",
    "            yield next(station)\n",
    "        time.sleep(sleep_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35373f3b-5280-4215-86ea-735f7678f4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stations-json', 'stations', '__consumer_offsets']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer, TopicPartition\n",
    "from kafka.admin import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError, UnknownTopicOrPartitionError\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers=[\"kafka:9092\"])\n",
    "try:\n",
    "    admin.delete_topics([\"stations\", \"stations-json\"])\n",
    "    print(\"deleted\")\n",
    "except UnknownTopicOrPartitionError:\n",
    "    print(\"cannot delete (may not exist yet)\")\n",
    "\n",
    "time.sleep(1)\n",
    "admin.create_topics([NewTopic(\"stations\", 6, 1)])\n",
    "admin.create_topics([NewTopic(\"stations-json\", 6, 1)])\n",
    "admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275a6d73-0d04-4622-99d2-ff974016d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building protobuf file\n",
    "! python3 -m grpc_tools.protoc -I=. --python_out=. stations.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e60c7dbc-2de2-45f9-8e60-1c6aa7319a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, threading\n",
    "from stations_pb2 import *\n",
    "\n",
    "def produce():\n",
    "    producer = KafkaProducer(bootstrap_servers=[\"kafka:9092\"], retries=10, acks='all')\n",
    "    \n",
    "    for date, station, degrees, raining in all_stations(15):\n",
    "        # protobuf\n",
    "        r = Report(date=date, station=station, degrees=degrees, raining=raining)\n",
    "        value1 = r.SerializeToString()\n",
    "        key = bytes(station, \"utf-8\")\n",
    "        producer.send(\"stations\", value=value1, key=key)\n",
    "        \n",
    "        # JSON\n",
    "        value2 = {\"date\":date, \"station\":station, \"degrees\":degrees, \"raining\":raining}\n",
    "        \n",
    "        if raining == False:\n",
    "            value2[\"raining\"] = 0\n",
    "        else:\n",
    "            value2[\"raining\"] = 1\n",
    "            \n",
    "        value2 = bytes(json.dumps(value2), \"utf-8\")\n",
    "        producer.send(\"stations-json\", value=value2, key=key)\n",
    "        \n",
    "# never join thread because we want it to run forever\n",
    "threading.Thread(target=produce, args=()).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68396e-aa0b-4f6a-bf51-e886f814dc56",
   "metadata": {},
   "source": [
    "# Part 2: Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c79677e-d0d6-448e-9f6d-fc672563e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "for partition in range(6):\n",
    "    path = f\"partition-{partition}.json\"\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568548b7-1214-4e1b-8dbf-3bc0a545c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and saving a partition\n",
    "def load_partition(partition_num):\n",
    "    path = f\"partition-{partition_num}.json\"\n",
    "    if os.path.exists(path):\n",
    "        print(\"TRUE\")\n",
    "        with open(path, \"r\") as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        return {\"offset\":None, \"partition\":partition_num}\n",
    "    \n",
    "def save_partition(partition):\n",
    "    path = f\"partition-{partition['partition']}.json\"\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump(partition, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a72c0035-b8f2-4873-84d5-9d43e829c434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND 0\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6 (consume):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_226/1061262091.py\", line 49, in consume\n",
      "NameError: name 'current_offset' is not defined\n",
      "Exception in thread Thread-7 (consume):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_226/1061262091.py\", line 49, in consume\n",
      "NameError: name 'current_offset' is not defined\n",
      "Exception in thread Thread-8 (consume):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_226/1061262091.py\", line 49, in consume\n",
      "NameError: name 'current_offset' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND 1\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n",
      "TRUE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9 (consume):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_226/1061262091.py\", line 49, in consume\n",
      "Exception in thread Thread-10 (consume):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "NameError: name 'current_offset' is not defined\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_226/1061262091.py\", line 49, in consume\n",
      "NameError: name 'current_offset' is not defined\n",
      "Exception in thread Thread-11 (consume):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_226/1061262091.py\", line 49, in consume\n",
      "NameError: name 'current_offset' is not defined\n"
     ]
    }
   ],
   "source": [
    "def consume(part_nums=[], iterations=10):\n",
    "    consumer = KafkaConsumer(bootstrap_servers=[\"kafka:9092\"])\n",
    "    consumer.assign([TopicPartition(\"stations\", part_num) for part_num in part_nums])\n",
    "\n",
    "    # PART 1: initialization\n",
    "    partitions = {} # key=partition num, value=snapshot dict\n",
    "    for partition_num in part_nums:\n",
    "        res = load_partition(partition_num)\n",
    "        if res[\"offset\"] == None:\n",
    "            consumer.seek_to_beginning()\n",
    "        else:\n",
    "            offset = res[\"offset\"]\n",
    "            consumer.seek(TopicPartition(\"stations\", partition_num), offset)\n",
    "        save_partition(res)\n",
    "        partitions[partition_num] = load_partition(partition_num)\n",
    "        \n",
    "    # PART 2: process batches\n",
    "    for i in range(iterations):\n",
    "        batch = consumer.poll(1000) # 1s timeout\n",
    "        for topic, messages in batch.items():\n",
    "            temp_count = 0\n",
    "            temp_sum = 0\n",
    "            for msg in messages:\n",
    "                r = Report.FromString(msg.value)\n",
    "                partition = msg.partition\n",
    "                station_key = r.station\n",
    "                temp = r.degrees\n",
    "                current_offset = consumer.position(topic)\n",
    "                \n",
    "                temp_count += 1\n",
    "                temp_sum += temp\n",
    "                # avg = temp_sum / temp_count\n",
    "                current_date = r.date\n",
    "\n",
    "                if station_key not in partitions[partition]:\n",
    "                    partitions[partition][station_key] = {}\n",
    "                    partitions[partition][station_key][\"start\"] = current_date\n",
    "                    partitions[partition][station_key][\"count\"] = temp_count\n",
    "                    partitions[partition][station_key][\"sum\"] = temp_sum\n",
    "                    partitions[partition][station_key][\"avg\"] = 0\n",
    "                else:\n",
    "                    previous_date = partitions[partition][station_key][\"end\"]\n",
    "                    if current_date <= previous_date:\n",
    "                        continue\n",
    "                    \n",
    "                partitions[partition][station_key][\"count\"] += temp_count\n",
    "                partitions[partition][station_key][\"sum\"] += temp_sum\n",
    "                partitions[partition][station_key][\"avg\"] = partitions[partition][station_key][\"sum\"] / partitions[partition][station_key][\"count\"]\n",
    "                partitions[partition][station_key][\"end\"] = current_date\n",
    "                partitions[partition][\"offset\"] = current_offset\n",
    "                \n",
    "        for part_num in partitions:\n",
    "            save_partition(partitions[partition])\n",
    "    print(\"exiting\")\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"ROUND\", i)\n",
    "    t1 = threading.Thread(target=consume, args=([0,1], 30))\n",
    "    t2 = threading.Thread(target=consume, args=([2,3], 30))\n",
    "    t3 = threading.Thread(target=consume, args=([4,5], 30))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7fcce6e-ebf6-4d75-a329-71a23b10d55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"offset\": 61, \"partition\": 0, \"N\": {\"start\": \"2000-01-01\", \"end\": \"2000-03-01\", \"avg\": 54.239378098600966, \"count\": 1, \"sum\": 54.239378098600966}}{\"offset\": 124, \"partition\": 1, \"E\": {\"start\": \"2000-01-01\", \"end\": \"2000-03-02\", \"avg\": 29.84173081380031, \"count\": 1, \"sum\": 29.84173081380031}, \"O\": {\"start\": \"2000-01-01\", \"end\": \"2000-03-02\", \"avg\": 33.29131050256177, \"count\": 2, \"sum\": 66.58262100512354}}{\"offset\": 180, \"partition\": 2, \"F\": {\"start\": \"2000-01-01\", \"end\": \"2000-02-29\", \"avg\": 31.970912694630492, \"count\": 1, \"sum\": 31.970912694630492}, \"I\": {\"start\": \"2000-01-01\", \"end\": \"2000-02-29\", \"avg\": 33.619682135409555, \"count\": 2, \"sum\": 67.23936427081911}, \"J\": {\"start\": \"2000-01-01\", \"end\": \"2000-02-29\", \"avg\": 30.661771334919717, \"count\": 3, \"sum\": 91.98531400475915}}{\"offset\": 189, \"partition\": 3, \"D\": {\"start\": \"2000-01-01\", \"end\": \"2000-03-03\", \"avg\": 50.80060022340461, \"count\": 1, \"sum\": 50.80060022340461}, \"G\": {\"start\": \"2000-01-01\", \"end\": \"2000-03-03\", \"avg\": 45.353979586847366, \"count\": 2, \"sum\": 90.70795917369473}, \"M\": {\"start\": \"2000-01-01\", \"end\": \"2000-03-03\", \"avg\": 46.750586030980855, \"count\": 3, \"sum\": 140.25175809294257}}{\"offset\": 320, \"partition\": 4, \"A\": {\"start\": \"2000-01-01\", \"end\": \"2000-03-04\", \"avg\": 23.240075787971307, \"count\": 1, \"sum\": 23.240075787971307}, \"B\": {\"start\": \"2000-01-01\", \"end\": \"2000-03-04\", \"avg\": 25.958739873988964, \"count\": 2, \"sum\": 51.91747974797793}, \"C\": {\"start\": \"2000-01-01\", \"end\": \"2000-03-04\", \"avg\": 22.990341839935923, \"count\": 3, \"sum\": 68.97102551980777}, \"K\": {\"start\": \"2000-01-01\", \"end\": \"2000-03-04\", \"avg\": 26.892188799202938, \"count\": 4, \"sum\": 107.56875519681175}, \"L\": {\"start\": \"2000-01-01\", \"end\": \"2000-03-04\", \"avg\": 26.893436666909487, \"count\": 5, \"sum\": 134.46718333454743}}{\"offset\": 63, \"partition\": 5, \"H\": {\"start\": \"2000-01-01\", \"end\": \"2000-03-03\", \"avg\": 23.293784493931884, \"count\": 1, \"sum\": 23.293784493931884}}"
     ]
    }
   ],
   "source": [
    "!cat partition*.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
