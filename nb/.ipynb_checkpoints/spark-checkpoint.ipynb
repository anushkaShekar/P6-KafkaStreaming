{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "373cfdaf-956c-4aee-aa1e-b58a0a8572cd",
   "metadata": {},
   "source": [
    "# Part 3: Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95606d26-de3a-4937-bdb5-19b5032f2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", 10)\n",
    "         .config(\"spark.ui.showConsoleProgress\", False)\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.2')\n",
    "         .getOrCreate())\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"stations-json\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a33b8847-7129-4a9a-9d13-fe3ad7008a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "\n",
    "schema = \"station STRING, date DATE, degrees DOUBLE, raining INT\"\n",
    "stations = (df.select(col(\"key\").cast(\"string\"),\n",
    "          from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "    .select(\"key\", \"value.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcafb716-65a5-4cfd-9203-db15f648a893",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e65a38-66bf-4eb5-a4a8-e915362a2e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[station: string, start: date, end: date, measurements: bigint, avg: double, max: double]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as functions\n",
    "\n",
    "counts_df = stations.groupBy(\"station\")\\\n",
    "                .agg(functions.min(\"date\").alias(\"start\"), \\\n",
    "                     functions.max(\"date\").alias(\"end\"), \\\n",
    "                     functions.count(\"*\").alias(\"measurements\"), \\\n",
    "                     functions.avg(\"degrees\").alias(\"avg\"), \\\n",
    "                     functions.max(\"degrees\").alias(\"max\")) \\\n",
    "                .sort(\"station\")\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79b868a-8027-4458-99be-e73b198e853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/27 18:14:55 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c3bf2db6-b62b-4358-99bc-f91c80640a0f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/27 18:14:55 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|     start|       end|measurements|               avg|              max|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|      A|2000-01-01|2000-04-01|          92|27.484061964548992|52.50510850992144|\n",
      "|      B|2000-01-01|2000-04-01|          92| 42.45665501320163|80.26884277730134|\n",
      "|      C|2000-01-01|2000-04-01|          92|41.240703885123274|73.87817340852196|\n",
      "|      D|2000-01-01|2000-04-01|          92|28.181801983563115|52.18858728283854|\n",
      "|      E|2000-01-01|2000-04-01|          92|27.556855284817047|56.43119385063466|\n",
      "|      F|2000-01-01|2000-04-01|          92| 49.18569199349005|68.60424517740242|\n",
      "|      G|2000-01-01|2000-04-01|          92|26.041105858477586|51.30926520080542|\n",
      "|      H|2000-01-01|2000-04-01|          92| 36.02207278907561|59.77849175827691|\n",
      "|      I|2000-01-01|2000-04-01|          92|41.284744581332205|66.17859127563213|\n",
      "|      J|2000-01-01|2000-04-01|          92|   36.213142425048|55.24306566401715|\n",
      "|      K|2000-01-01|2000-04-01|          92| 42.67140480823132|71.99379917263178|\n",
      "|      L|2000-01-01|2000-04-01|          92| 29.51342960052234|49.98110263022937|\n",
      "|      M|2000-01-01|2000-04-01|          92|43.361173798899785| 69.2532975796779|\n",
      "|      N|2000-01-01|2000-04-01|          92| 43.35963297808748|67.58361419097932|\n",
      "|      O|2000-01-01|2000-04-01|          92| 41.60550518929138|78.94525912635461|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/27 18:15:12 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 15882 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      A|2000-01-01|2000-04-12|         103|29.254329731599395| 53.07958331419695|\n",
      "|      B|2000-01-01|2000-04-12|         103|45.215158346343884|   82.503942209696|\n",
      "|      C|2000-01-01|2000-04-12|         103|41.983238439087245| 73.87817340852196|\n",
      "|      D|2000-01-01|2000-04-12|         103| 31.56362212585022| 70.84373980312505|\n",
      "|      E|2000-01-01|2000-04-12|         103| 30.14528824664039| 60.00186780638552|\n",
      "|      F|2000-01-01|2000-04-12|         103| 49.25002579602891| 68.60424517740242|\n",
      "|      G|2000-01-01|2000-04-12|         103|28.965797894056774|60.076972744479775|\n",
      "|      H|2000-01-01|2000-04-12|         103| 36.73443103348826| 59.77849175827691|\n",
      "|      I|2000-01-01|2000-04-12|         103| 42.85676833147984| 66.17859127563213|\n",
      "|      J|2000-01-01|2000-04-12|         103| 38.24882582114767| 72.62872883497053|\n",
      "|      K|2000-01-01|2000-04-12|         103|  44.1419350027896| 71.99379917263178|\n",
      "|      L|2000-01-01|2000-04-12|         103| 33.21907256735721| 71.13819100716661|\n",
      "|      M|2000-01-01|2000-04-12|         103| 44.99239077808456|  76.0933188254894|\n",
      "|      N|2000-01-01|2000-04-12|         103| 45.77868248499254| 74.47555746348715|\n",
      "|      O|2000-01-01|2000-04-12|         103| 43.92709079407058| 78.94525912635461|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      A|2000-01-01|2000-04-15|         106| 30.05242876402206|61.247236121799574|\n",
      "|      B|2000-01-01|2000-04-15|         106| 46.09582434467774|   82.503942209696|\n",
      "|      C|2000-01-01|2000-04-15|         106| 41.88327743257809| 73.87817340852196|\n",
      "|      D|2000-01-01|2000-04-15|         106| 32.00573063573164| 70.84373980312505|\n",
      "|      E|2000-01-01|2000-04-15|         106|30.765873845782092| 60.00186780638552|\n",
      "|      F|2000-01-01|2000-04-15|         106| 49.76501555779528| 74.41802600605733|\n",
      "|      G|2000-01-01|2000-04-15|         106| 29.67939439158635|60.076972744479775|\n",
      "|      H|2000-01-01|2000-04-15|         106| 37.13223038705939| 59.77849175827691|\n",
      "|      I|2000-01-01|2000-04-15|         106|  43.1250727655779| 66.17859127563213|\n",
      "|      J|2000-01-01|2000-04-15|         106| 38.52047617518343| 72.62872883497053|\n",
      "|      K|2000-01-01|2000-04-15|         106| 44.71114632808915| 71.99379917263178|\n",
      "|      L|2000-01-01|2000-04-15|         106| 33.88623717419172| 71.13819100716661|\n",
      "|      M|2000-01-01|2000-04-15|         106|45.048233170737205|  76.0933188254894|\n",
      "|      N|2000-01-01|2000-04-15|         106|46.729929162185186|  83.3111801792317|\n",
      "|      O|2000-01-01|2000-04-15|         106| 44.93952872091557| 85.72721318164928|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      A|2000-01-01|2000-04-19|         110|30.912498550746243|61.247236121799574|\n",
      "|      B|2000-01-01|2000-04-19|         110|47.284341052151156| 85.07975771284481|\n",
      "|      C|2000-01-01|2000-04-19|         110| 42.23290526798801| 73.87817340852196|\n",
      "|      D|2000-01-01|2000-04-19|         110| 32.81661369619272| 70.84373980312505|\n",
      "|      E|2000-01-01|2000-04-19|         110|31.663549000564206| 60.00186780638552|\n",
      "|      F|2000-01-01|2000-04-19|         110| 50.32444305144575|  76.8315854155955|\n",
      "|      G|2000-01-01|2000-04-19|         110|30.305707507445604|60.076972744479775|\n",
      "|      H|2000-01-01|2000-04-19|         110| 38.20153623549599| 72.01286082851425|\n",
      "|      I|2000-01-01|2000-04-19|         110| 43.69152298625022| 66.17859127563213|\n",
      "|      J|2000-01-01|2000-04-19|         110| 39.44683828167571| 72.62872883497053|\n",
      "|      K|2000-01-01|2000-04-19|         110|  45.7967551091417| 77.54371547436271|\n",
      "|      L|2000-01-01|2000-04-19|         110| 34.38230888106439| 71.13819100716661|\n",
      "|      M|2000-01-01|2000-04-19|         110| 45.20500464157579|  76.0933188254894|\n",
      "|      N|2000-01-01|2000-04-19|         110|48.156996435119034| 93.02442581957654|\n",
      "|      O|2000-01-01|2000-04-19|         110|46.165026166208776| 87.58782623931917|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/27 18:15:26 WARN TaskSetManager: Lost task 4.0 in stage 20.0 (TID 148) (a370f3c803f3 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/27 18:15:26 WARN TaskSetManager: Lost task 5.0 in stage 20.0 (TID 149) (a370f3c803f3 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "s = counts_df.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").outputMode(\"complete\").start()\n",
    "s.awaitTermination(30)\n",
    "s.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7515d1-b2b2-4903-8ac8-c3f60a6b7c68",
   "metadata": {},
   "source": [
    "## Rain Forecast Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "71b96ebb-a591-4b38-81db-33dd23c27a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating today DataFrame\n",
    "today = stations.select(\"station\", \"date\", \"raining\")\n",
    "\n",
    "# t = today.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").outputMode(\"Append\").start()\n",
    "# t.awaitTermination(10)\n",
    "# t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "11bb1d1b-8175-4558-89d2-b335d38f508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating features DataFrame\n",
    "df_yesterday = stations.select(\"station\", \n",
    "                               col(\"degrees\").alias(\"sub1degrees\"), \\\n",
    "                               col(\"raining\").alias(\"sub1raining\"), \\\n",
    "                               functions.date_add(\"date\", 1).alias(\"date\"))\n",
    "\n",
    "df_two_days_ago = stations.select(\"station\", \n",
    "                                  col(\"degrees\").alias(\"sub2degrees\"), \\\n",
    "                                  col(\"raining\").alias(\"sub2raining\"), \\\n",
    "                                  functions.date_add(\"date\", 2).alias(\"date\"))\n",
    "features = df_yesterday.join(df_two_days_ago, \\\n",
    "                             (df_yesterday.date == df_two_days_ago.date) & (df_yesterday.station == df_two_days_ago.station)) \\\n",
    "                    .select(df_yesterday.station, df_yesterday.date, functions.month(df_yesterday.date).alias(\"month\"), \"sub1degrees\", \"sub1raining\", \"sub2degrees\", \"sub2raining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3aa8d451-f549-472b-afe7-8c0db81b46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/27 22:18:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/04/27 22:18:13 ERROR MicroBatchExecution: Query [id = 3bc31b0f-50c0-4933-aa8f-4cf157624417, runId = 53093f13-94c7-43bb-adfa-073ad17d9a78] terminated with error\n",
      "org.apache.spark.sql.AnalysisException: Stream-stream join without equality predicate is not supported;\n",
      "Join Inner\n",
      ":- Project [station#5520, date#7126, month(date#7126) AS month#7158, sub1degrees#7124, sub1raining#7125, sub2degrees#7131, sub2raining#7132]\n",
      ":  +- Join Inner, ((date#7126 = date#7133) AND (station#5520 = station#7138))\n",
      ":     :- Project [from_json(StructField(station,StringType,true), cast(value#5503 as string), Some(GMT)).station AS station#5520, from_json(StructField(degrees,DoubleType,true), cast(value#5503 as string), Some(GMT)).degrees AS sub1degrees#7124, from_json(StructField(raining,IntegerType,true), cast(value#5503 as string), Some(GMT)).raining AS sub1raining#7125, date_add(from_json(StructField(date,DateType,true), cast(value#5503 as string), Some(GMT)).date, 1) AS date#7126]\n",
      ":     :  +- Filter (isnotnull(date_add(from_json(StructField(date,DateType,true), cast(value#5503 as string), Some(GMT)).date, 1)) AND isnotnull(from_json(StructField(station,StringType,true), cast(value#5503 as string), Some(GMT)).station))\n",
      ":     :     +- StreamingDataSourceV2Relation [key#5502, value#5503, topic#5504, partition#5505, offset#5506L, timestamp#5507, timestampType#5508], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@6fdc514b, KafkaV2[Subscribe[stations-json]], {\"stations-json\":{\"2\":0,\"5\":0,\"4\":0,\"1\":0,\"3\":0,\"0\":0}}, {\"stations-json\":{\"2\":10626,\"5\":3542,\"4\":17710,\"1\":7084,\"3\":10626,\"0\":3542}}\n",
      ":     +- Project [from_json(StructField(station,StringType,true), cast(value#5503 as string), Some(GMT)).station AS station#7138, from_json(StructField(degrees,DoubleType,true), cast(value#5503 as string), Some(GMT)).degrees AS sub2degrees#7131, from_json(StructField(raining,IntegerType,true), cast(value#5503 as string), Some(GMT)).raining AS sub2raining#7132, date_add(from_json(StructField(date,DateType,true), cast(value#5503 as string), Some(GMT)).date, 2) AS date#7133]\n",
      ":        +- Filter (isnotnull(date_add(from_json(StructField(date,DateType,true), cast(value#5503 as string), Some(GMT)).date, 2)) AND isnotnull(from_json(StructField(station,StringType,true), cast(value#5503 as string), Some(GMT)).station))\n",
      ":           +- StreamingDataSourceV2Relation [key#5502, value#5503, topic#5504, partition#5505, offset#5506L, timestamp#5507, timestampType#5508], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@6fdc514b, KafkaV2[Subscribe[stations-json]], {\"stations-json\":{\"2\":0,\"5\":0,\"4\":0,\"1\":0,\"3\":0,\"0\":0}}, {\"stations-json\":{\"2\":10626,\"5\":3542,\"4\":17710,\"1\":7084,\"3\":10626,\"0\":3542}}\n",
      "+- Project [from_json(StructField(station,StringType,true), cast(value#5503 as string), Some(GMT)).station AS station#7226, from_json(StructField(date,DateType,true), cast(value#5503 as string), Some(GMT)).date AS date#7227, from_json(StructField(raining,IntegerType,true), cast(value#5503 as string), Some(GMT)).raining AS raining#7229]\n",
      "   +- StreamingDataSourceV2Relation [key#5502, value#5503, topic#5504, partition#5505, offset#5506L, timestamp#5507, timestampType#5508], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@6fdc514b, KafkaV2[Subscribe[stations-json]], {\"stations-json\":{\"2\":0,\"5\":0,\"4\":0,\"1\":0,\"3\":0,\"0\":0}}, {\"stations-json\":{\"2\":10626,\"5\":3542,\"4\":17710,\"1\":7084,\"3\":10626,\"0\":3542}}\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.streamJoinStreamWithoutEqualityPredicateUnsupportedError(QueryCompilationErrors.scala:1329)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies$StreamingJoinStrategy$.apply(SparkStrategies.scala:429)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:453)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:144)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:183)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:183)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:144)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:137)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:157)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:183)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:183)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:157)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:591)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:581)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n"
     ]
    }
   ],
   "source": [
    "new_df = features.join(today) \n",
    "#new_df = features.join(today, (features.date == today.date) & (features.station == today.station)) \n",
    "s1 = (\n",
    "    new_df\n",
    "    .repartition(1)\n",
    "    .writeStream.format(\"parquet\")\n",
    "    .outputMode(\"Append\")\n",
    "    .option(\"checkpointLocation\", \"/notebooks/checkpoint\")\n",
    "    .option(\"path\", \"/notebooks/station-files\")\n",
    "    .trigger(processingTime=\"60 seconds\")\n",
    "    .start()\n",
    ")\n",
    "s1.awaitTermination(10)\n",
    "s1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e7bde900-c5f3-447f-95c4-97f9bd9b057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = features.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").outputMode(\"Append\").start()\n",
    "# s.awaitTermination(30)\n",
    "# s.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a0659-ad30-4d9c-af36-349a492bf3d4",
   "metadata": {},
   "source": [
    "# Part 4: Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b94529-324a-4507-a459-6bf8b171885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcae026-90fa-407c-8f6d-ddea625e80d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
