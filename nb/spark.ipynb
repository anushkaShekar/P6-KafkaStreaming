{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "373cfdaf-956c-4aee-aa1e-b58a0a8572cd",
   "metadata": {},
   "source": [
    "# Part 3: Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95606d26-de3a-4937-bdb5-19b5032f2d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-51e536d0-6b73-494c-a882-c504634e91fe;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1215ms :: artifacts dl 61ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-51e536d0-6b73-494c-a882-c504634e91fe\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/20ms)\n",
      "23/04/29 04:20:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", 10)\n",
    "         .config(\"spark.ui.showConsoleProgress\", False)\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.2')\n",
    "         .getOrCreate())\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"stations-json\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a33b8847-7129-4a9a-9d13-fe3ad7008a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "\n",
    "schema = \"station STRING, date DATE, degrees DOUBLE, raining INT\"\n",
    "stations = (df.select(col(\"key\").cast(\"string\"),\n",
    "          from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "    .select(\"key\", \"value.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcafb716-65a5-4cfd-9203-db15f648a893",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e65a38-66bf-4eb5-a4a8-e915362a2e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[station: string, start: date, end: date, measurements: bigint, avg: double, max: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as functions\n",
    "\n",
    "counts_df = stations.groupBy(\"station\")\\\n",
    "                .agg(functions.min(\"date\").alias(\"start\"), \\\n",
    "                     functions.max(\"date\").alias(\"end\"), \\\n",
    "                     functions.count(\"*\").alias(\"measurements\"), \\\n",
    "                     functions.avg(\"degrees\").alias(\"avg\"), \\\n",
    "                     functions.max(\"degrees\").alias(\"max\")) \\\n",
    "                .sort(\"station\")\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f79b868a-8027-4458-99be-e73b198e853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/29 04:25:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9722e7dc-c846-48b6-b216-88dd38911ebe. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/29 04:25:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      A|2000-01-01|2019-06-02|        7093|56.668704420792686|107.44151918167982|\n",
      "|      B|2000-01-01|2019-06-02|        7093| 48.10064640296037| 99.58872422458987|\n",
      "|      C|2000-01-01|2019-06-02|        7093| 43.94465266851925|  93.4385638974453|\n",
      "|      D|2000-01-01|2019-06-02|        7093| 63.15489726310973|112.74755001728609|\n",
      "|      E|2000-01-01|2019-06-02|        7093| 57.13525259560014| 110.0060171182283|\n",
      "|      F|2000-01-01|2019-06-02|        7093| 68.53567341979365|122.13936667629562|\n",
      "|      G|2000-01-01|2019-06-02|        7093| 67.61963924542304|121.37722118686678|\n",
      "|      H|2000-01-01|2019-06-02|        7093| 67.36593873868541| 117.3104489218558|\n",
      "|      I|2000-01-01|2019-06-02|        7093| 62.53285936615791|124.58760542249914|\n",
      "|      J|2000-01-01|2019-06-02|        7093|48.996871754807586|102.87171771509877|\n",
      "|      K|2000-01-01|2019-06-02|        7093|47.603998436892084| 96.94806911086113|\n",
      "|      L|2000-01-01|2019-06-02|        7093| 62.39891590936878| 113.5867887931967|\n",
      "|      M|2000-01-01|2019-06-02|        7093| 63.92229391066125|113.25066131629002|\n",
      "|      N|2000-01-01|2019-06-02|        7093| 68.94568768144958| 123.1374930839637|\n",
      "|      O|2000-01-01|2019-06-02|        7093| 69.73081710836755|128.10700147395923|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/29 04:25:51 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 7697 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      A|2000-01-01|2019-06-07|        7098|56.685522070683334|107.44151918167982|\n",
      "|      B|2000-01-01|2019-06-07|        7098| 48.11131828093756| 99.58872422458987|\n",
      "|      C|2000-01-01|2019-06-07|        7098| 43.96138812007781|  93.4385638974453|\n",
      "|      D|2000-01-01|2019-06-07|        7098|  63.1714622293232|112.74755001728609|\n",
      "|      E|2000-01-01|2019-06-07|        7098| 57.15407008549074| 110.0060171182283|\n",
      "|      F|2000-01-01|2019-06-07|        7098| 68.54157883115742|122.13936667629562|\n",
      "|      G|2000-01-01|2019-06-07|        7098| 67.62998916057417|121.37722118686678|\n",
      "|      H|2000-01-01|2019-06-07|        7098|  67.3791004958546| 117.3104489218558|\n",
      "|      I|2000-01-01|2019-06-07|        7098|  62.5518053950377|124.58760542249914|\n",
      "|      J|2000-01-01|2019-06-07|        7098| 49.00860326879991|102.87171771509877|\n",
      "|      K|2000-01-01|2019-06-07|        7098| 47.61752645765133| 96.94806911086113|\n",
      "|      L|2000-01-01|2019-06-07|        7098| 62.41652441181725| 113.5867887931967|\n",
      "|      M|2000-01-01|2019-06-07|        7098| 63.93704935661839|113.25066131629002|\n",
      "|      N|2000-01-01|2019-06-07|        7098| 68.94764209896282| 123.1374930839637|\n",
      "|      O|2000-01-01|2019-06-07|        7098| 69.74834016523295|128.10700147395923|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      A|2000-01-01|2019-06-10|        7101| 56.69760433449759|107.44151918167982|\n",
      "|      B|2000-01-01|2019-06-10|        7101| 48.12140212747956| 99.58872422458987|\n",
      "|      C|2000-01-01|2019-06-10|        7101| 43.97025188411517|  93.4385638974453|\n",
      "|      D|2000-01-01|2019-06-10|        7101|63.183281200239044|112.74755001728609|\n",
      "|      E|2000-01-01|2019-06-10|        7101|57.159656872292175| 110.0060171182283|\n",
      "|      F|2000-01-01|2019-06-10|        7101| 68.54749524914557|122.13936667629562|\n",
      "|      G|2000-01-01|2019-06-10|        7101| 67.63800885731217|121.37722118686678|\n",
      "|      H|2000-01-01|2019-06-10|        7101| 67.38424354938725| 117.3104489218558|\n",
      "|      I|2000-01-01|2019-06-10|        7101|62.558559385424466|124.58760542249914|\n",
      "|      J|2000-01-01|2019-06-10|        7101| 49.01776973463964|102.87171771509877|\n",
      "|      K|2000-01-01|2019-06-10|        7101|  47.6301249370049| 96.94806911086113|\n",
      "|      L|2000-01-01|2019-06-10|        7101| 62.43199631508366| 113.5867887931967|\n",
      "|      M|2000-01-01|2019-06-10|        7101| 63.94679980111076|113.25066131629002|\n",
      "|      N|2000-01-01|2019-06-10|        7101| 68.94775452393482| 123.1374930839637|\n",
      "|      O|2000-01-01|2019-06-10|        7101|  69.7554091808565|128.10700147395923|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      A|2000-01-01|2019-06-15|        7106| 56.71584195920713|107.44151918167982|\n",
      "|      B|2000-01-01|2019-06-15|        7106|48.138707758169566| 99.58872422458987|\n",
      "|      C|2000-01-01|2019-06-15|        7106| 43.98256633805369|  93.4385638974453|\n",
      "|      D|2000-01-01|2019-06-15|        7106| 63.20022772282692|112.74755001728609|\n",
      "|      E|2000-01-01|2019-06-15|        7106| 57.17368180684236| 110.0060171182283|\n",
      "|      F|2000-01-01|2019-06-15|        7106| 68.56790585982814|122.13936667629562|\n",
      "|      G|2000-01-01|2019-06-15|        7106| 67.65112603885837|121.37722118686678|\n",
      "|      H|2000-01-01|2019-06-15|        7106| 67.40183866389418| 117.3104489218558|\n",
      "|      I|2000-01-01|2019-06-15|        7106| 62.57185143238835|124.58760542249914|\n",
      "|      J|2000-01-01|2019-06-15|        7106|49.030628211726885|102.87171771509877|\n",
      "|      K|2000-01-01|2019-06-15|        7106| 47.63727328416275| 96.94806911086113|\n",
      "|      L|2000-01-01|2019-06-15|        7106| 62.45043463660686| 113.5867887931967|\n",
      "|      M|2000-01-01|2019-06-15|        7106| 63.96008217324104|113.25066131629002|\n",
      "|      N|2000-01-01|2019-06-15|        7106| 68.95949966419846| 123.1374930839637|\n",
      "|      O|2000-01-01|2019-06-15|        7106|  69.7713254559955|128.10700147395923|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      A|2000-01-01|2019-06-20|        7111| 56.73531031410661|107.44151918167982|\n",
      "|      B|2000-01-01|2019-06-20|        7111| 48.16111672918766| 99.58872422458987|\n",
      "|      C|2000-01-01|2019-06-20|        7111|43.997583649752315|  93.4385638974453|\n",
      "|      D|2000-01-01|2019-06-20|        7111|63.214690787407065|112.74755001728609|\n",
      "|      E|2000-01-01|2019-06-20|        7111|  57.1888439426031| 110.0060171182283|\n",
      "|      F|2000-01-01|2019-06-20|        7111| 68.60046907102446|122.13936667629562|\n",
      "|      G|2000-01-01|2019-06-20|        7111| 67.67095582329453|121.37722118686678|\n",
      "|      H|2000-01-01|2019-06-20|        7111| 67.41678431462738| 117.3104489218558|\n",
      "|      I|2000-01-01|2019-06-20|        7111| 62.59002685470409|124.58760542249914|\n",
      "|      J|2000-01-01|2019-06-20|        7111| 49.04990209996486|102.87171771509877|\n",
      "|      K|2000-01-01|2019-06-20|        7111|47.651670984023404| 96.94806911086113|\n",
      "|      L|2000-01-01|2019-06-20|        7111|62.467278433483294| 113.5867887931967|\n",
      "|      M|2000-01-01|2019-06-20|        7111| 63.97057398984127|113.25066131629002|\n",
      "|      N|2000-01-01|2019-06-20|        7111| 68.97261752436941| 123.1374930839637|\n",
      "|      O|2000-01-01|2019-06-20|        7111| 69.78114858546131|128.10700147395923|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|     start|       end|measurements|               avg|               max|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|      A|2000-01-01|2019-06-25|        7116| 56.75711403242704|107.44151918167982|\n",
      "|      B|2000-01-01|2019-06-25|        7116| 48.18799486905864| 99.58872422458987|\n",
      "|      C|2000-01-01|2019-06-25|        7116| 44.01262198940409|  93.4385638974453|\n",
      "|      D|2000-01-01|2019-06-25|        7116|63.223347900190745|112.74755001728609|\n",
      "|      E|2000-01-01|2019-06-25|        7116|57.204259251043716| 110.0060171182283|\n",
      "|      F|2000-01-01|2019-06-25|        7116| 68.62899197059028|122.13936667629562|\n",
      "|      G|2000-01-01|2019-06-25|        7116| 67.68884308132948|121.37722118686678|\n",
      "|      H|2000-01-01|2019-06-25|        7116| 67.43844406519534| 117.3104489218558|\n",
      "|      I|2000-01-01|2019-06-25|        7116| 62.61086110597032|124.58760542249914|\n",
      "|      J|2000-01-01|2019-06-25|        7116|49.062674175604386|102.87171771509877|\n",
      "|      K|2000-01-01|2019-06-25|        7116| 47.66765423175403| 96.94806911086113|\n",
      "|      L|2000-01-01|2019-06-25|        7116| 62.48493127874592| 113.5867887931967|\n",
      "|      M|2000-01-01|2019-06-25|        7116| 63.98659583063652|113.25066131629002|\n",
      "|      N|2000-01-01|2019-06-25|        7116| 68.98863418469202| 123.1374930839637|\n",
      "|      O|2000-01-01|2019-06-25|        7116|  69.7936684333835|128.10700147395923|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = counts_df.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").outputMode(\"complete\").start()\n",
    "s.awaitTermination(30)\n",
    "s.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7515d1-b2b2-4903-8ac8-c3f60a6b7c68",
   "metadata": {},
   "source": [
    "## Rain Forecast Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71b96ebb-a591-4b38-81db-33dd23c27a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating today DataFrame\n",
    "today = stations.select(\"station\", \"date\", \"raining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11bb1d1b-8175-4558-89d2-b335d38f508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating features DataFrame\n",
    "df_yesterday = stations.select(\"station\",\n",
    "                               col(\"degrees\").alias(\"sub1degrees\"), \\\n",
    "                               col(\"raining\").alias(\"sub1raining\"), \\\n",
    "                               functions.date_add(\"date\", 1).alias(\"date\"))\n",
    "\n",
    "df_two_days_ago = stations.select(\"station\",\n",
    "                                  col(\"degrees\").alias(\"sub2degrees\"), \\\n",
    "                                  col(\"raining\").alias(\"sub2raining\"), \\\n",
    "                                  functions.date_add(\"date\", 2).alias(\"date\"))\n",
    "features = df_yesterday.join(df_two_days_ago, \\\n",
    "                             (df_yesterday.date == df_two_days_ago.date) & (df_yesterday.station == df_two_days_ago.station)) \\\n",
    "                    .select(df_yesterday.station, df_yesterday.date, functions.month(df_yesterday.date).alias(\"month\"), \"sub1degrees\", \"sub1raining\", \"sub2degrees\", \"sub2raining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa8d451-f549-472b-afe7-8c0db81b46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/29 04:26:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "new_df = features.join(today, [\"date\", \"station\"])\n",
    "\n",
    "s1 = (\n",
    "    new_df\n",
    "    .repartition(1)\n",
    "    .writeStream.format(\"parquet\")\n",
    "    .outputMode(\"Append\")\n",
    "    .option(\"checkpointLocation\", \"checkpoint\")\n",
    "    .option(\"path\", \"pfiles\")\n",
    "    .trigger(processingTime=\"60 seconds\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a0659-ad30-4d9c-af36-349a492bf3d4",
   "metadata": {},
   "source": [
    "# Part 4: Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0c0576-7331-438c-ae6f-2b2873fc4c8f",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57b94529-324a-4507-a459-6bf8b171885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdcae026-90fa-407c-8f6d-ddea625e80d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.format(\"parquet\").load(\"pfiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b422480-0657-4001-9a94-b9b5c4ed1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=[\"month\", \"sub1degrees\", \"sub1raining\", \"sub2degrees\", \"sub2raining\"], outputCol=\"features\")\n",
    "data = va.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9140ce3a-4838-404e-adef-b4f56329938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d53b9cdf-a6b0-4b94-b6f2-926586eff9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_8c21bd7bd8d1, depth=5, numNodes=17, numClasses=2, numFeatures=5\n",
      "  If (feature 2 <= 0.5)\n",
      "   Predict: 0.0\n",
      "  Else (feature 2 > 0.5)\n",
      "   If (feature 1 <= 39.6116668769335)\n",
      "    If (feature 1 <= 36.913191004253875)\n",
      "     If (feature 0 <= 2.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 0 > 2.5)\n",
      "      If (feature 1 <= 34.20731431135777)\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 > 34.20731431135777)\n",
      "       Predict: 1.0\n",
      "    Else (feature 1 > 36.913191004253875)\n",
      "     If (feature 0 <= 1.5)\n",
      "      If (feature 3 <= 45.125769741185344)\n",
      "       Predict: 0.0\n",
      "      Else (feature 3 > 45.125769741185344)\n",
      "       Predict: 1.0\n",
      "     Else (feature 0 > 1.5)\n",
      "      If (feature 3 <= 27.288838352542765)\n",
      "       Predict: 0.0\n",
      "      Else (feature 3 > 27.288838352542765)\n",
      "       Predict: 1.0\n",
      "   Else (feature 1 > 39.6116668769335)\n",
      "    Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"raining\", predictionCol=\"prediction\", maxDepth=5)\n",
    "dt_model = dt_classifier.fit(train_data)\n",
    "print(dt_model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df81ab14-cf18-49bb-bdf1-edd57149b8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7871331969860532\n",
      "Percent of the time it is raining: 0.3531218483542679\n"
     ]
    }
   ],
   "source": [
    "predictions = dt_model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"raining\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "rain_count = test_data.select().filter(test_data.raining == 1).count()\n",
    "total_count = test_data.select(test_data.raining).count()\n",
    "rain_frequency = rain_count / total_count\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Percent of the time it is raining:\", rain_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663effc-3443-4d06-b11b-a53c0f5d77fd",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23fa8705-47c1-4752-9b42-0d194d2318a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = va.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31330488-1d9d-4e33-a030-0317f005b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/29 04:36:34 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-42c65849-b9c0-4284-8596-47500cbf53e3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/29 04:36:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2000-01-13|       0.0|\n",
      "|      A|2000-01-21|       0.0|\n",
      "|      A|2000-01-23|       0.0|\n",
      "|      A|2000-01-28|       0.0|\n",
      "|      A|2000-02-01|       0.0|\n",
      "|      A|2000-02-11|       0.0|\n",
      "|      A|2000-02-16|       0.0|\n",
      "|      A|2000-02-19|       0.0|\n",
      "|      A|2000-02-21|       0.0|\n",
      "|      A|2000-03-01|       0.0|\n",
      "|      A|2000-03-04|       0.0|\n",
      "|      A|2000-03-25|       1.0|\n",
      "|      A|2000-03-29|       1.0|\n",
      "|      A|2000-04-21|       0.0|\n",
      "|      A|2000-04-25|       1.0|\n",
      "|      A|2000-05-10|       0.0|\n",
      "|      A|2000-05-22|       1.0|\n",
      "|      A|2000-06-01|       1.0|\n",
      "|      A|2000-06-04|       1.0|\n",
      "|      A|2000-07-01|       1.0|\n",
      "+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/29 04:36:48 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 13858 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2021-03-21|       1.0|\n",
      "|      A|2021-03-22|       1.0|\n",
      "|      A|2021-03-23|       0.0|\n",
      "|      A|2021-03-17|       0.0|\n",
      "|      A|2021-03-14|       1.0|\n",
      "|      A|2021-03-20|       1.0|\n",
      "|      A|2021-03-19|       1.0|\n",
      "|      A|2021-03-18|       1.0|\n",
      "|      A|2021-03-13|       1.0|\n",
      "|      A|2021-03-15|       1.0|\n",
      "|      A|2021-03-16|       1.0|\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2021-03-24|       0.0|\n",
      "|      A|2021-03-26|       1.0|\n",
      "|      A|2021-03-25|       0.0|\n",
      "|      A|2021-03-27|       1.0|\n",
      "|      A|2021-03-28|       1.0|\n",
      "+-------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/29 04:36:59 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 6199 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2021-03-29|       1.0|\n",
      "|      A|2021-04-01|       1.0|\n",
      "|      A|2021-03-30|       1.0|\n",
      "|      A|2021-03-31|       1.0|\n",
      "|      A|2021-04-02|       1.0|\n",
      "|      A|2021-04-03|       1.0|\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2021-04-07|       0.0|\n",
      "|      A|2021-04-06|       0.0|\n",
      "|      A|2021-04-04|       0.0|\n",
      "|      A|2021-04-08|       1.0|\n",
      "|      A|2021-04-05|       0.0|\n",
      "+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_predictions = dt_model.transform(features).select(\"station\", \"date\", \"prediction\").filter(\"station='A'\")\n",
    "s2 = weather_predictions.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").start()\n",
    "s2.awaitTermination(40)\n",
    "s2.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
